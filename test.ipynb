{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from random import randrange, seed\n",
    "from csv import reader\n",
    "from math import sqrt, exp\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row: continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def str_column_to_flt(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "\n",
    "\n",
    "def str_column_to_int(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = int(row[column])\n",
    "\n",
    "\n",
    "\n",
    "def accuracy_metric(actual, predicted,temp_conf_1):\n",
    "    global temp_conf\n",
    "    correct = 0\n",
    "    actual_final.append(actual)\n",
    "    predicted_final.append(predicted)\n",
    "    for i1 in range(len(actual)):\n",
    "\n",
    "        if actual[i1] == predicted[i1]: \n",
    "            correct += 1\n",
    "\n",
    "          \n",
    "    cm = confusion_matrix(actual,predicted)\n",
    "    cm_n = np.float64(cm)\n",
    "\n",
    "           \n",
    "    temp_conf = temp_conf + cm_n\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    print(\"***Before Normalized Confusion Matrix***\",temp_conf)\n",
    "       \n",
    "\n",
    "\n",
    "    return 100. * correct  / float(len(actual))\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_algorithm(dataset, algorithm, *args):\n",
    "\n",
    "\n",
    "    test_set = subsample_test(dataset, 1 - sample_ratio)\n",
    "\n",
    "\n",
    "    predicted = algorithm(test_set, *args)\n",
    "    actual = [row[-2] for row in test_set]\n",
    "\n",
    "\n",
    "    accuracy = accuracy_metric(actual, predicted,temp_conf)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = list(), list()\n",
    "\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "\n",
    "\n",
    "def gini_index(groups, class_values):\n",
    "    gini = 0.0\n",
    "    for class_value in class_values:\n",
    "        for group in groups:\n",
    "            size = len(group)\n",
    "            if size == 0: continue\n",
    "            proportion = [row[-2] for row in group].count(class_value) / float(size)\n",
    "            gini += (proportion * (1.0 - proportion))\n",
    "    return gini\n",
    "\n",
    "\n",
    "def gain_index(groups, classes, H_Y):\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    gain = 0.0\n",
    "    split = 0.0\n",
    "    ratio = 0.0\n",
    "    score = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        if size == 0:\n",
    "            continue\n",
    "\n",
    "        for class_val in classes:\n",
    "            p = [row[-2] for row in group].count(class_val) / float(size)\n",
    "            if p == 0.0:\n",
    "                continue\n",
    "            score = p*math.log(p,2)\n",
    "            gain += score * (size / n_instances)\n",
    "\n",
    "        \n",
    "        split = split - (size/n_instances)* math.log(size/n_instances, 2)\n",
    "\n",
    "    gain = H_Y + gain\n",
    "    if split!=0:\n",
    "        ratio = gain/split\n",
    "    return ratio\n",
    "\n",
    "\n",
    "\n",
    "def get_split(dataset, n_features):\n",
    "    class_values = list(set(row[-2] for row in dataset))\n",
    "    \n",
    "    b_index, b_value, b_score, b_groups = 0,0,-999, None\n",
    "    \n",
    "    v = float((len(dataset[0]))-1)\n",
    "    temp = 0\n",
    "    for class_val in class_values:\n",
    "        p = [row[-2] for row in dataset].count(class_val) / ((np.size(dataset)) / v)\n",
    "        if p == 0:\n",
    "            continue\n",
    "        temp = temp + p * math.log(p, 2)\n",
    "    H_Y = (-1) * temp\n",
    "    \n",
    "    features = list()\n",
    "    while len(features) < n_features:\n",
    "        index = randrange(len(dataset[0]) - 2)\n",
    "        if index not in features: features.append(index)\n",
    "\n",
    "    for index in features:\n",
    "\n",
    "        high=(len(dataset) - 1)\n",
    "        rv = randrange(0, high)\n",
    "        groups = test_split(index, dataset[rv][index], dataset)\n",
    "        \n",
    "        gain = gain_index(groups, class_values, H_Y)\n",
    "        \n",
    "        if gain > b_score:\n",
    "            b_index, b_value, b_score, b_groups = index, dataset[rv][index], gain, groups\n",
    "\n",
    "    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
    "\n",
    "\n",
    "\n",
    "def to_terminal(group, n_classes):\n",
    "    class_no = [0]*n_classes\n",
    "    for row in group:\n",
    "        no = row[-2]\n",
    "        class_no[no]+=1\n",
    "    return class_no\n",
    "\n",
    "\n",
    "def check_terminate(arr):\n",
    "    count = 0\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i] != 0:\n",
    "            count += 1\n",
    "\n",
    "    if count == 1:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def split(node, max_depth, min_size, n_features, depth, n_classes):\n",
    "    left, right = node['groups']\n",
    "    del (node['groups'])\n",
    "    if depth > max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left + right, n_classes), to_terminal(left + right, n_classes)\n",
    "        return\n",
    "\n",
    "    if len(left) >= min_size:\n",
    "        temp = to_terminal(left,n_classes)\n",
    "        if check_terminate(temp):\n",
    "            node['left'] = temp\n",
    "\n",
    "        else:\n",
    "            node['left'] = get_split(left, n_features)\n",
    "            split(node['left'], max_depth, min_size, n_features, depth + 1, n_classes)\n",
    "\n",
    "    if len(left) < min_size:\n",
    "        node['left'] = to_terminal(left, n_classes)\n",
    "\n",
    "    if len(right) >= min_size:\n",
    "        temp = to_terminal(right, n_classes)\n",
    "        if check_terminate(temp):\n",
    "            node['right'] = temp\n",
    "\n",
    "        else:\n",
    "            node['right'] = get_split(right, n_features)\n",
    "            split(node['right'], max_depth, min_size, n_features, depth + 1, n_classes)\n",
    "\n",
    "    if len(right) < min_size:\n",
    "        node['right'] = to_terminal(right, n_classes)\n",
    "\n",
    "\n",
    "\n",
    "def build_tree(train1, max_depth, min_size, n_features, n_classes):\n",
    "    root = get_split(train1, n_features)\n",
    "    split(root, max_depth, min_size, n_features, 1, n_classes)\n",
    "\n",
    "    return root\n",
    "\n",
    "\n",
    "\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "\n",
    "\n",
    "def subsample_test(dataset, ratio):\n",
    "    sample = list()\n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    while len(sample) < n_sample:\n",
    "        index = randrange(len(dataset))\n",
    "        if dataset[index] not in sample:\n",
    "            sample.append(dataset[index])\n",
    "    return sample\n",
    "\n",
    "\n",
    "\n",
    "def subsample_train(dataset, test_data, ratio):\n",
    "    sample = list()\n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    while len(sample) < n_sample:\n",
    "        index = randrange(len(dataset))\n",
    "        if dataset[index] not in test_data:\n",
    "            sample.append(dataset[index])\n",
    "    return sample\n",
    "\n",
    "\n",
    "\n",
    "def bagging_predict(trees, row, n_classes,exponent_value):\n",
    "    scores = list()\n",
    "    tree_prob = list()\n",
    "    distance = list()\n",
    "    for tree in trees:\n",
    "        scores, tree_prob, distance = test_knn(tree, row, scores, tree_prob, n_classes,exponent_value, distance)\n",
    "    \n",
    "    sum_scores = float(sum(scores))\n",
    "    for i6 in range(len(scores)):\n",
    "        scores[i6] = scores[i6]/sum_scores\n",
    "\n",
    "\n",
    "    for i7 in range(len(scores)):\n",
    "        for j in range(len(tree_prob[i7])):\n",
    "            tree_prob[i7][j] = tree_prob[i7][j]*scores[i7]\n",
    "\n",
    "    final_predict = [0]*n_classes\n",
    "\n",
    "    for i8 in range(n_classes):\n",
    "        sum_classes = 0\n",
    "        for i9 in range(len(scores)):\n",
    "            sum_classes += tree_prob[i9][i8]\n",
    "        final_predict[i8] = sum_classes\n",
    "\n",
    "    ind = final_predict.index(max(final_predict))\n",
    "    return ind\n",
    "\n",
    "\n",
    "\n",
    "def random_forest(test, max_depth, min_size, n_trees, n_features, sample_ratio, n_classes,exponent_value,temp_conf):\n",
    "\n",
    "    predicted = list()\n",
    "\n",
    "\n",
    "    for i3 in range(n_trees):\n",
    "        train = subsample_train(dataset, test, sample_ratio)\n",
    "        tree = build_tree(train, max_depth, min_size, n_features, n_classes)\n",
    "        trees.append(tree)\n",
    "\n",
    "    for row in test:\n",
    "        predict = bagging_predict(trees, row,n_classes,exponent_value)\n",
    "        file.write(str(predict)+\",\"+str(row[-1])+\"\\n\")\n",
    "        predicted.append(predict)\n",
    "\n",
    "\n",
    "    return predicted\n",
    "\n",
    "\n",
    "def depth(d, level=1):\n",
    "    if not isinstance(d, dict) or not d:\n",
    "        return level\n",
    "    return max(depth(d[k], level + 1) for k in d)\n",
    "\n",
    "\n",
    "def test_knn(tree, test_data, scores, tree_prob, n_classes, exponent_value,distance):\n",
    "    root = tree\n",
    "    prob = [0]*n_classes\n",
    "    dist_knn = 0\n",
    "    while (isinstance(root, dict)):\n",
    "        ind = int(root['index'])\n",
    "        dist = test_data[ind] - root['value']\n",
    "        dist_knn += dist * dist\n",
    "        if (dist >= 0):\n",
    "            root = root['right']\n",
    "        else:\n",
    "            root = root['left']\n",
    "\n",
    "    distance.append(dist_knn)\n",
    "    score = exp(-sqrt(dist_knn)/exponent_value)\n",
    "    scores.append(score)\n",
    "\n",
    "    ave_sum = float(sum(root))\n",
    "    if ave_sum !=0:\n",
    "        for i4 in range(len(root)):\n",
    "            prob[i4] = root[i4]/ave_sum\n",
    "\n",
    "    index = prob.index(max(prob))\n",
    "    for j in range(len(prob)):\n",
    "        if j != index:\n",
    "            prob[j] = 0.0\n",
    "        else:\n",
    "            prob[j] = 1.0\n",
    "\n",
    "\n",
    "    tree_prob.append(prob)\n",
    "    return scores, tree_prob, distance\n",
    "\n",
    "\n",
    "def data_normalize(dataset):\n",
    "    for i5 in range(len(dataset[0])-2):\n",
    "        inz = [row[i5] for row in dataset]\n",
    "        sum_mean = max(inz) - min(inz)\n",
    "\n",
    "        if(sum_mean != 0):\n",
    "            for k in range(len(dataset)):\n",
    "                dataset[k][i5] = (dataset[k][i5] - min(inz)) / sum_mean\n",
    "    return dataset\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Before Normalized Confusion Matrix*** [[15.  0.  0.]\n",
      " [ 0. 19.  3.]\n",
      " [ 0.  2.  6.]]\n",
      "Accuracy after 1 Iteration: 88.88888888888889\n",
      "***Before Normalized Confusion Matrix*** [[29.  0.  0.]\n",
      " [ 0. 36.  4.]\n",
      " [ 0.  4. 17.]]\n",
      "Accuracy after 2 Iteration: 93.33333333333333\n",
      "***Before Normalized Confusion Matrix*** [[42.  0.  0.]\n",
      " [ 0. 58.  8.]\n",
      " [ 0.  4. 23.]]\n",
      "Accuracy after 3 Iteration: 91.11111111111111\n",
      "***Before Normalized Confusion Matrix*** [[59.  0.  0.]\n",
      " [ 0. 77.  9.]\n",
      " [ 0.  6. 29.]]\n",
      "Accuracy after 4 Iteration: 93.33333333333333\n",
      "***Before Normalized Confusion Matrix*** [[76.  0.  0.]\n",
      " [ 0. 95. 10.]\n",
      " [ 0.  7. 37.]]\n",
      "Accuracy after 5 Iteration: 95.55555555555556\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "dataset = load_csv('./iris.data.csv')\n",
    "for i in range(0, (len(dataset[0])-1)):\n",
    "    str_column_to_flt(dataset, i)\n",
    "\n",
    "str_column_to_int(dataset, (len(dataset[0]) - 2))\n",
    "\n",
    "dataset = data_normalize(dataset)\n",
    "\n",
    "i = len(dataset[0])-2\n",
    "\n",
    "\n",
    "trees = list()\n",
    "\n",
    "max_depth = 25\n",
    "min_size = 10\n",
    "Max_times = 5\n",
    "sample_ratio = 0.7\n",
    "n_classes = 3\n",
    "\n",
    "\n",
    "temp_conf = np.zeros((3,3))\n",
    "\n",
    "actual_final = list()\n",
    "predicted_final = list()\n",
    "\n",
    "exponent_value = 0.75\n",
    "score_array = np.array([0.0] * Max_times)\n",
    "n_features = int(sqrt(len(dataset[0]) - 2)) + 1\n",
    "seed(123)\n",
    "\n",
    "\n",
    "for n_trees in [45]:\n",
    "    for j in range(1, Max_times + 1):\n",
    "        filename='Iris_predict_'+str(j)+'.csv'\n",
    "        file = open(filename, 'a')        \n",
    "        k = j\n",
    "\n",
    "        scores = evaluate_algorithm(dataset, random_forest, max_depth, min_size, n_trees, n_features, sample_ratio, n_classes,exponent_value,temp_conf)\n",
    "        score_array[k - 1] = scores\n",
    "        print(\"Accuracy after\",j,\"Iteration:\",scores)\n",
    "        file.write(str(scores)+\"\\n\")\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        76\n",
      "           1       0.93      0.90      0.92       105\n",
      "           2       0.79      0.84      0.81        44\n",
      "\n",
      "    accuracy                           0.92       225\n",
      "   macro avg       0.91      0.92      0.91       225\n",
      "weighted avg       0.93      0.92      0.93       225\n",
      "\n",
      "***Normalized Confusion Matrix***\n",
      "[[1.         0.         0.        ]\n",
      " [0.         0.9047619  0.0952381 ]\n",
      " [0.         0.15909091 0.84090909]]\n",
      "Total Execution Time: 121.42500686645508\n",
      "No of trees: 45\n",
      "Max Times: 5\n",
      "Max depth: 25\n",
      "min_size: 10\n",
      "train test ratio: 0.7\n",
      "exponent_value: 0.75\n",
      "******Mean***** : 92.44444444444444\n",
      "****Standard Deviation ****: 2.266230894930127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "a_list = [y for x in actual_final for y in x]\n",
    "p_list = [y for x in predicted_final for y in x]\n",
    "conf = confusion_matrix(a_list, p_list)\n",
    "\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(a_list, p_list))\n",
    "\n",
    "\n",
    "temp_conf = conf.astype(float)  \n",
    "row_count = np.zeros(len(temp_conf))\n",
    "for l in range(len(temp_conf)):\n",
    "    for m in range(len(temp_conf)):\n",
    "        row_count[l] = row_count[l] + temp_conf[l][m]\n",
    "\n",
    "for l in range(len(temp_conf)):\n",
    "    for m in range(len(temp_conf)):\n",
    "        temp_conf[l][m] = float(temp_conf[l][m] / float(row_count[l]))\n",
    "\n",
    "print(\"***Normalized Confusion Matrix***\")\n",
    "print(temp_conf)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total Execution Time:\", end - start)\n",
    "print(\"No of trees:\", n_trees)\n",
    "print(\"Max Times:\", Max_times)\n",
    "print(\"Max depth:\", max_depth)\n",
    "print(\"min_size:\", min_size)\n",
    "print(\"train test ratio:\", sample_ratio)\n",
    "print(\"exponent_value:\", exponent_value)\n",
    "\n",
    "\n",
    "print('******Mean***** :', np.mean(score_array))\n",
    "print('****Standard Deviation ****:', np.std(score_array))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
